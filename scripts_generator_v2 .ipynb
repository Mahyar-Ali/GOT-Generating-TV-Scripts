{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scripts-generator_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzCrYrrr1Zp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrqTHfdl-CoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_lookup():\n",
        "    mark_to_token={}\n",
        "    mark_to_token['.'] = \"||period||\"\n",
        "    mark_to_token[','] = \"||comma||\"\n",
        "    mark_to_token['\"'] = \"||uotationmark||\"\n",
        "    mark_to_token[';'] = \"||semicolon||\"\n",
        "    mark_to_token['!'] = \"||exclamationmark||\"\n",
        "    mark_to_token['?'] = \"||questionmark||\"\n",
        "    mark_to_token['('] = \"||leftparentheses||\"\n",
        "    mark_to_token[')'] = \"||rightparentheses||\"\n",
        "    mark_to_token['--'] = \"||dash||\"\n",
        "    mark_to_token['\\n'] = \"||return||\"\n",
        "    mark_to_token['“'] = \"||quotationMark||\"\n",
        "    mark_to_token['”'] = \"||quotationMark||\"\n",
        "    return mark_to_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prec5BCJ-Dtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lookup_tables(text):\n",
        "      #text is split into words\n",
        "      words = sorted(list(set(text)))\n",
        "      vocab_to_int = {word:index for index,word in enumerate(words)}\n",
        "      int_to_vocab = np.array(words)\n",
        "      return vocab_to_int,int_to_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs9NBWH_-U5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(token_lookup,create_lookup_tables):\n",
        "    #Reading all the scripts\n",
        "\n",
        "    text1 = open(\"got1.txt\",'r').read()\n",
        "    text2 = open(\"got2.txt\",'r').read()\n",
        "    text3 = open(\"got3.txt\",'r').read()\n",
        "    text4 = open(\"got4.txt\",'r').read()\n",
        "    text5 = open(\"got5.txt\",'r').read()\n",
        "\n",
        "    text = text1+\"\\n\\n\"+text2+\"\\n\\n\"+text3+\"\\n\\n\"+text4+\"\\n\\n\"+text5+\"\\n\\n\"\n",
        "\n",
        "    token_dict = token_lookup()\n",
        "\n",
        "    for key,token in token_dict.items():\n",
        "      text = text.replace(key,' {} '.format(token))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    word2idx,idx2word = create_lookup_tables(text)\n",
        "\n",
        "    words_as_int = [word2idx[word] for word in text]\n",
        "\n",
        "    return words_as_int,word2idx,idx2word,token_dict,text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKMXe-vG_3lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_as_int,word2idx,idx2word,token_dict,words = preprocess_data(token_lookup,create_lookup_tables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFFjUq1K2BVA",
        "colab_type": "code",
        "outputId": "f3418d12-d614-4446-b4ab-e3023b84c8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Total Number of Unique Words {}\".format(len(word2idx)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Unique Words 26687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnENOoHs2T6D",
        "colab_type": "code",
        "outputId": "dc959492-9687-489b-d213-ccd67a7cbdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Cross Check\n",
        "text = open(\"got5.txt\",'r').read()\n",
        "print(text[:35])\n",
        "print(words[:7])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "“We should start back,” Gared urged\n",
            "['||return||', 'the', 'comet’s', 'tail', 'spread', 'across', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdMIT-O835I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 50\n",
        "\n",
        "words_dataset = tf.data.Dataset.from_tensor_slices(words_as_int) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGrPDEaL4Bdy",
        "colab_type": "code",
        "outputId": "6ac68888-f910-4328-d24e-c97c0e79091e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "for i in words_dataset.take(5):\n",
        "  print(idx2word[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "||return||\n",
            "the\n",
            "comet’s\n",
            "tail\n",
            "spread\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LYEDxFJ5g9x",
        "colab_type": "code",
        "outputId": "8e5a2650-c088-4322-fe39-f19418f6ec89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "sequences = words_dataset.batch(seq_length+1,drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(2):\n",
        "  print(repr(' '.join(idx2word[seq.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'||return|| the comet’s tail spread across the dawn ||comma|| a red slash that bled above the crags of dragonstone like a wound in the pink and purple sky ||period|| ||return|| ||return|| the maester stood on the windswept balcony outside his chambers ||period|| it was here the ravens came ||comma|| after long'\n",
            "'flight ||period|| their droppings speckled the gargoyles that rose twelve feet tall on either side of him ||comma|| a hellhound and a wyvern ||comma|| two of the thousand that brooded over the walls of the ancient fortress ||period|| when first he came to dragonstone ||comma|| the army of stone grotesques had'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAE3upe6HNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text,target_text\n",
        "\n",
        "batches = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dngjj5l77q0",
        "colab_type": "code",
        "outputId": "f26f0bd2-c00a-4976-c041-c260063d3936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "for input_example,target_example in batches.take(1):\n",
        "  print(\"Input data:\",repr(\" \".join(idx2word[input_example.numpy()])))\n",
        "  print(\"Target data:\",repr(\" \".join(idx2word[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: '||return|| the comet’s tail spread across the dawn ||comma|| a red slash that bled above the crags of dragonstone like a wound in the pink and purple sky ||period|| ||return|| ||return|| the maester stood on the windswept balcony outside his chambers ||period|| it was here the ravens came ||comma|| after'\n",
            "Target data: 'the comet’s tail spread across the dawn ||comma|| a red slash that bled above the crags of dragonstone like a wound in the pink and purple sky ||period|| ||return|| ||return|| the maester stood on the windswept balcony outside his chambers ||period|| it was here the ravens came ||comma|| after long'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjuZp3lMHi8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "56d6dc22-dd2a-43de-9bef-f9f823547906"
      },
      "source": [
        "Batch_Size = 128\n",
        "\n",
        "Buffer_Size= 1000\n",
        "\n",
        "dataset = batches.shuffle(Buffer_Size).batch(Batch_Size,drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 50), (128, 50)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXI4eDXhIBNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2idx)\n",
        "\n",
        "embedding_size = 400\n",
        "\n",
        "rnn_units = [512,256]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0mqynHOIOoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_Size,embedding_size,rnn_units,batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_Size,embedding_size,\n",
        "                                batch_input_shape=[batch_size,None]),\n",
        "      tf.keras.layers.LSTM(rnn_units[0],recurrent_initializer='glorot_uniform',\n",
        "                           return_sequences=True,stateful=True,recurrent_dropout=0.15),\n",
        "      tf.keras.layers.LSTM(rnn_units[1],recurrent_initializer='glorot_uniform',\n",
        "                           return_sequences=True,stateful=True,recurrent_dropout=0.1),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdNjklomNHau",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "d6e0cac4-65b5-4fe3-9159-2072c96810a1"
      },
      "source": [
        "model = build_model(vocab_size,embedding_size,rnn_units,128)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUd5-HkTNT3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "10b219bb-0d3d-47c8-f008-4473321e6180"
      },
      "source": [
        "for input_example_batch,input_target_batch in dataset.take(1):\n",
        "  example_batch_prediction = model(input_example_batch)\n",
        "  print(example_batch_prediction.shape)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 50, 26687)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tedNAgSANvtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "a180b8ca-ecad-4d98-9e3b-3248fac547c4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (128, None, 400)          10674800  \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (128, None, 512)          1869824   \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (128, None, 256)          787456    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (128, None, 26687)        6858559   \n",
            "=================================================================\n",
            "Total params: 20,190,639\n",
            "Trainable params: 20,190,639\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1lrgkAGNylW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_indices = tf.random.categorical(example_batch_prediction[0],num_samples=1)\n",
        "sampled_indices= tf.squeeze(sample_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53XQZbDAOIoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "0054b250-d7eb-415d-ff2a-d24502b5f0fb"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   96, 12538, 11171, 18879,  6488, 11466, 13348, 14843,  2813,\n",
              "       21572, 24122, 16380, 19012, 13688, 25938, 25486, 17907,  5281,\n",
              "       25145,  1481, 20780,  7958,  4901,  4979, 19757, 19865,  9757,\n",
              "       18637,  6060, 19399, 10834, 17920,   303,  1653, 12822,  4265,\n",
              "        7479,  4734,  9686,  4311, 19131,  5974, 26537,  7181, 21722,\n",
              "       13422, 20936,   406, 18909,  1644])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un0H50DjOQC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "a0784a57-521f-45fd-e967-6f7101db74d3"
      },
      "source": [
        "print(\" \".join(idx2word[sampled_indices]))\n",
        "#As you can see that model just outputs gibberish,"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "access laggardly hugged rookery dripped impulse loutish namesake bryen—didn’t squish tyrion petting rue— manes wine-sodden weighs ravine dare waits basted sluggishly firmly crewed crossbowman senses seven-times-damned gritted riddle dissemble scapegoat hisself rayder affirmation been—she legitimacy compounded fact cower greyguard’s condolences s-s-simple dishes —to escorted statuary lumpier snarling— aisles rosby’s beech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKBZfieWl47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_prefix = os.path.join(\"model\",'ckpt_{epoch}')\n",
        "chkp = tf.keras.callbacks.ModelCheckpoint(checkpoint_prefix,save_weights_only=True,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDZz82mxZObU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF3gMiXROUGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    clipped_grads = [tf.clip_by_value(grad,-2.,2.) for grad in grads if grad is not None]\n",
        "    optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x73PdbtLVbOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(\"model\"))\n",
        "EPOCHS=200\n",
        "for epoch in range(EPOCHS):\n",
        "  start=time.time()\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n,(inp,target)) in enumerate(dataset):\n",
        "\n",
        "    loss = train_step(inp,target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "        template = 'Epoch {} Batch {} Loss {}'\n",
        "        print(template.format(epoch+1, batch_n, loss))\n",
        "    \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsvZRJfAskNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Batch_Size = 1\n",
        "\n",
        "model = build_model(vocab_size,embedding_size,rnn_units,Batch_Size)\n",
        "model.load_weights(tf.train.latest_checkpoint('model'))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWHZ6lr-wsno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "1b917809-b7a3-4f88-c44f-8e5d8f6a3a44"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 400)            10674800  \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (1, None, 512)            1869824   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (1, None, 256)            787456    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 26687)          6858559   \n",
            "=================================================================\n",
            "Total params: 20,190,639\n",
            "Trainable params: 20,190,639\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi7gILCOXGYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model,start_string):\n",
        "  #Number of words to generate\n",
        "  num_generate = 40\n",
        "  #Converting start string to numbers\n",
        "  input_eval = [word2idx[s] for s in start_string.split()]\n",
        "  \n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  temperature =1.0\n",
        "\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    prediction = model(input_eval)\n",
        "\n",
        "    prediction =tf.squeeze(prediction,0)\n",
        "\n",
        "    prediction= prediction/temperature\n",
        "\n",
        "    predicted_id = tf.random.categorical(prediction,num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval=tf.expand_dims([predicted_id],0)\n",
        "\n",
        "    text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "  text = start_string+\" \".join(text_generated)\n",
        "\n",
        "\n",
        "  #I am genrating the token dict again :\n",
        "  def token_lookup():\n",
        "    mark_to_token={}\n",
        "    mark_to_token['.'] = \"||period||\"\n",
        "    mark_to_token[','] = \"||comma||\"\n",
        "    mark_to_token['\"'] = \"||quotationmark||\"\n",
        "    mark_to_token[';'] = \"||semicolon||\"\n",
        "    mark_to_token['!'] = \"||exclamationmark||\"\n",
        "    mark_to_token['?'] = \"||questionmark||\"\n",
        "    mark_to_token['('] = \"||leftparentheses||\"\n",
        "    mark_to_token[')'] = \"||rightparentheses||\"\n",
        "    mark_to_token['--'] = \"||dash||\"\n",
        "    mark_to_token['\\n'] = \"||return||\"\n",
        "    return mark_to_token\n",
        "\n",
        "  #Conerting the tokens back to their markers\n",
        "  token_dict = token_lookup()\n",
        "  for key,token in token_dict.items():\n",
        "    text = text.replace(token,key)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz_-hu-TtRuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = generate_text(model,start_string=u\"jon\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO-pHdEUz4wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}