{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TV-Scripts Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oZY3AP_1f-Yj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_znO5juAfgcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHQ7PMzgfxoa",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkW0tRlVV54H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helper\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkxFd3-WUA3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "7ec8f86c-f6ec-488e-c83a-0d54f72ca602"
      },
      "source": [
        "!unzip 'game-of-thrones-book-files.zip'"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  game-of-thrones-book-files.zip\n",
            "  inflating: got1.txt                \n",
            "  inflating: got2.txt                \n",
            "  inflating: got3.txt                \n",
            "  inflating: got4.txt                \n",
            "  inflating: got5.txt                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peaEUSblbAsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "569fbfaf-ca27-42a9-870f-f7245252c8ad"
      },
      "source": [
        "#Reading the Script\n",
        "with open(\"got1.txt\",'r') as file:\n",
        "  got1=file.read()\n",
        "with open(\"got2.txt\",'r') as file:\n",
        "  got2=file.read()\n",
        "with open(\"got3.txt\",'r') as file:\n",
        "  got3=file.read()\n",
        "with open(\"got1.txt\",'r') as file:\n",
        "  got4=file.read()\n",
        "with open(\"got1.txt\",'r') as file:\n",
        "  got5=file.read()\n",
        "\n",
        "text = got1+got2+got3+got4+got5\n",
        "print(len(text))\n",
        "print(len(got1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9702721\n",
            "1733704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSRYdc0-bWdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "c50959e1-3691-4537-acdc-0e909f1de66d"
      },
      "source": [
        "view_sentence_range = (0, 10)\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "scenes = text.split('\\n\\n')\n",
        "print('Number of scenes: {}'.format(len(scenes)))\n",
        "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
        "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
        "\n",
        "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
        "print('Number of lines: {}'.format(len(sentences)))\n",
        "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
        "\n",
        "print()\n",
        "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
        "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 56308\n",
            "Number of scenes: 42815\n",
            "Average number of sentences in each scene: 0.0017984351278757444\n",
            "Number of lines: 42892\n",
            "Average number of words in each line: 41.90688240231279\n",
            "\n",
            "The sentences 0 to 10:\n",
            "\n",
            "The comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like a wound in the pink and purple sky.\n",
            "\n",
            "The maester stood on the windswept balcony outside his chambers. It was here the ravens came, after long flight. Their droppings speckled the gargoyles that rose twelve feet tall on either side of him, a hellhound and a wyvern, two of the thousand that brooded over the walls of the ancient fortress. When first he came to Dragonstone, the army of stone grotesques had made him uneasy, but as the years passed he had grown used to them. Now he thought of them as old friends. The three of them watched the sky together with foreboding.\n",
            "\n",
            "The maester did not believe in omens. And yet . . . old as he was, Cressen had never seen a comet half so bright, nor yet that color, that terrible color, the color of blood and flame and sunsets. He wondered if his gargoyles had ever seen its like. They had been here so much longer than he had, and would still be here long after he was gone. If stone tongues could speak . . .\n",
            "\n",
            "Such folly. He leaned against the battlement, the sea crashing beneath him, the black stone rough beneath his fingers. Talking gargoyles and prophecies in the sky. I am an old done man, grown giddy as a child again. Had a lifetime’s hard-won wisdom fled him along with his health and strength? He was a maester, trained and chained in the great Citadel of Oldtown. What had he come to, when superstition filled his head as if he were an ignorant fieldhand?\n",
            "\n",
            "And yet . . . and yet . . . the comet burned even by day now, while pale grey steam rose from the hot vents of Dragonmont behind the castle, and yestermorn a white raven had brought word from the Citadel itself, word long-expected but no less fearful for all that, word of summer’s end. Omens, all. Too many to deny. What does it all mean? he wanted to cry.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8-MAMnodbCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_lookup_tables(text):\n",
        "      #text is split into words\n",
        "      words = sorted(list(set(text)))\n",
        "      vocab_to_int = {word:index for index,word in enumerate(words)}\n",
        "      int_to_vocab = {index:word for index,word in enumerate(words)}\n",
        "      return vocab_to_int,int_to_vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRX_Qe6ZfERJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_lookup():\n",
        "    mark_to_token={}\n",
        "    mark_to_token['.'] = \"||Period||\"\n",
        "    mark_to_token[','] = \"||Comma||\"\n",
        "    mark_to_token['\"'] = \"||QuotationMark||\"\n",
        "    mark_to_token[';'] = \"||Semicolon||\"\n",
        "    mark_to_token['!'] = \"||Exclamationmark||\"\n",
        "    mark_to_token['?'] = \"||Questionmark||\"\n",
        "    mark_to_token['('] = \"||LeftParentheses||\"\n",
        "    mark_to_token[')'] = \"||RightParentheses||\"\n",
        "    mark_to_token['--'] = \"||Dash||\"\n",
        "    mark_to_token['\\n'] = \"||Return||\"\n",
        "    return mark_to_token\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuSM05AQiVVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
        "    import pickle\n",
        "    \"\"\"\n",
        "    Preprocess Text Data\n",
        "    \"\"\"\n",
        "    with open(\"got1.txt\",'r') as file:\n",
        "      got1=file.read()\n",
        "    with open(\"got2.txt\",'r') as file:\n",
        "      got2=file.read()\n",
        "    with open(\"got3.txt\",'r') as file:\n",
        "      got3=file.read()\n",
        "    with open(\"got1.txt\",'r') as file:\n",
        "      got4=file.read()\n",
        "    with open(\"got1.txt\",'r') as file:\n",
        "      got5=file.read()\n",
        "\n",
        "    text = got1+got2+got3+got4+got5\n",
        "    \n",
        "\n",
        "    token_dict = token_lookup()\n",
        "    for key, token in token_dict.items():\n",
        "        text = text.replace(key, ' {} '.format(token))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
        "    int_text = [vocab_to_int[word] for word in text]\n",
        "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "she8ZHoupMv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess_and_save_data(\"none\",token_lookup,create_lookup_tables)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QFq1q08Zod-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "582dd176-a341-4999-b230-9f6d5de4ce93"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGlFQfjp3oE",
        "colab_type": "text"
      },
      "source": [
        "**Now our checkpoint has been created**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eubaXNs5qyab",
        "colab_type": "text"
      },
      "source": [
        "# Building The Netwok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpy6oE3Qp2nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNCNqBD1q7fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TibNH41orD4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs():\n",
        "  Inputs = tf.placeholder(tf.int32,[None,None],name='input')\n",
        "  Targets = tf.placeholder(tf.int32,[None,None],name='Targets')\n",
        "  LearningRate= tf.placeholder(tf.float32,name='learningrate')\n",
        "  return Inputs,Targets,LearningRate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgF-eAyIxlw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_cell(batch_size,rnn_size):\n",
        "  def getCell():\n",
        "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(rnn_cell,output_keep_prob=0.9)\n",
        "    return drop\n",
        "\n",
        "  cell = tf.contrib.rnn.MultiRNNCell([getCell() for i in range(2)])\n",
        "  initial_state = cell.zero_state(batch_size,tf.float32)\n",
        "  initial_state =  tf.identity(initial_state,\"initial_state\")\n",
        "  return cell,initial_state\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuqj6VQh4H4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embed(input_data,vocab_size,embed_dim):\n",
        "  embeddings = tf.Variable(tf.truncated_normal([vocab_size,embed_dim]))\n",
        "  embed = tf.nn.embedding_lookup(embeddings,input_data)\n",
        "  return embed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_RyO2LdBQ3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn(cell,inputs):\n",
        "  outputs,final_state  = tf.nn.dynamic_rnn(cell,inputs,dtype=tf.float32)\n",
        "  final_state = tf.identity(final_state,name='final_state')\n",
        "\n",
        "  return (outputs,final_state)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cyw6JUvDxvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_nn(cell,rnn_size,input_data,vocab_size,embed_dim):\n",
        "  embed = get_embed(input_data,vocab_size,embed_dim)\n",
        "  (outputs,final_state)  = build_rnn(cell,embed)\n",
        "\n",
        "  Logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=tf.keras.activations.linear)\n",
        "\n",
        "  return (Logits,final_state)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbc_ZdJhGOyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(int_text,batch_size,seq_len):\n",
        "  chars_per_batch = batch_size*seq_len\n",
        "  num_batches = len(int_text)//chars_per_batch\n",
        "  int_text = int_text[:num_batches*chars_per_batch]\n",
        "  batches = np.zeros([num_batches,2,batch_size,seq_len])\n",
        "  counter = 0\n",
        "  for i in range(batch_size):\n",
        "      for j in range(0,num_batches):\n",
        "            batches[j,0,i,:] = int_text[counter:counter+seq_len]\n",
        "            y_temp = int_text[counter+1:counter+seq_len+1]\n",
        "            batches[j,1,i,:len(y_temp)] = y_temp            \n",
        "            counter+=seq_len\n",
        "  batches[num_batches-1,1,-1,-1] = int_text[0]\n",
        "  return batches            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQjfrayNhZfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters\n",
        "\n",
        "num_epochs = 1\n",
        "batch_size=  256\n",
        "rnn_size = 300\n",
        "embed_dims = 300 \n",
        "seq_len = 42\n",
        "learning_rate = 0.001\n",
        "show_every_n_batches = 50\n",
        "\n",
        "save_dir = './save'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEs5Qst5mcQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the Graph\n",
        "from tensorflow.contrib import seq2seq\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "  vocab_size = len(int_to_vocab)\n",
        "  input_text,targets,lr = get_inputs()\n",
        "  input_data_shape = tf.shape(input_text)\n",
        "  cell,initial_state = get_init_cell(input_data_shape[0],rnn_size)\n",
        "  Logits,Final_state =   build_nn(cell,rnn_size,input_text,vocab_size,embed_dims)\n",
        "\n",
        "  probs = tf.nn.softmax(Logits,name='probs')\n",
        "  #Loss Function\n",
        "  cost = seq2seq.sequence_loss(Logits,targets,\n",
        "                               tf.ones([input_data_shape[0],input_data_shape[1]]))\n",
        "  \n",
        "  #optimizer\n",
        "  optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "  gradients=optimizer.compute_gradients(cost)\n",
        "\n",
        "  clipped_gradients = [(tf.clip_by_value(grad,-1.,1.),var) for grad,var in gradients if grad is not None]\n",
        "\n",
        "  train_op = optimizer.apply_gradients(clipped_gradients)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_heeS7Tlqim3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "50d11554-17b9-49ce-dc62-96a4460a36de"
      },
      "source": [
        "#Training the Network\n",
        "batches = get_batches(int_text,batch_size,seq_len)\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "  loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "  loader.restore(sess, load_dir)\n",
        "  #sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for epoch_i in range(num_epochs):\n",
        "      state = sess.run(initial_state,{input_text: batches[0][0]})\n",
        "      for batch_i,(x,y) in enumerate(batches):\n",
        "           feed = {input_text:x,\n",
        "                  targets:y,\n",
        "                  lr:learning_rate,\n",
        "                  initial_state:state}\n",
        "\n",
        "           train_loss,state,_ = sess.run([cost,Final_state,train_op],feed_dict=feed)\n",
        "           if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "  # Save Model\n",
        "  saver = tf.train.Saver()\n",
        "  saver.save(sess, save_dir)\n",
        "  print('Model Trained and Saved')\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n",
            "Epoch   0 Batch    0/205   train_loss = 6.410\n",
            "Epoch   0 Batch   50/205   train_loss = 6.392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-51a435820ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                   initial_state:state}\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m            \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo5ZJiHPPH9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import helper\n",
        "helper.save_params((seq_len, save_dir))\n",
        "\n",
        "#pickle.dump((seq_len,save_dir), open('params.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJYil1X23MrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "846dce2d-0f29-4c1c-9a59-46a629fb2bdf"
      },
      "source": [
        "#Checkpoint\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import helper\n",
        "\n",
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = helper.load_params()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFcrEpnx3tyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "897adb88-47cd-4bbd-e8f7-8d9ed6ee949e"
      },
      "source": [
        "def get_tensors(loaded_graph):\n",
        "  with tf.Session(graph=loaded_graph):\n",
        "      return (tf.get_default_graph().get_tensor_by_name('input:0')\n",
        "      ,tf.get_default_graph().get_tensor_by_name(\"initial_state:0\")\n",
        "      ,tf.get_default_graph().get_tensor_by_name('final_state:0')\n",
        "      ,tf.get_default_graph().get_tensor_by_name('probs:0') )\n",
        "  \n",
        "test_get_tensors(get_tensors)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSOCshAlB6_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d1723da1-3e8d-49e7-a1a0-b56f613d003d"
      },
      "source": [
        "def pick_word(probabilities,int_to_vocab,top_n=5):\n",
        "      p  = np.squeeze(probabilities)\n",
        "      vocab_size = len(int_to_vocab)\n",
        "      p[np.argsort(p)[:-top_n]] = 0\n",
        "      p = p / np.sum(p)\n",
        "      c = np.random.choice(vocab_size, 1, p=p)[0]\n",
        "      return int_to_vocab[c]\n",
        "test_pick_word(pick_word)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1SuxflTHDXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "dc34ec9b-a27e-4c57-9ad9-67e9bc0e9ad1"
      },
      "source": [
        "gen_length = 200\n",
        "\n",
        "prime_word = 'comet'\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "  loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "  loader.restore(sess, load_dir)\n",
        "\n",
        "  input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "\n",
        "  gen_sentences = [prime_word+':']\n",
        "  prev_state = sess.run(initial_state,{input_text:np.array([[1]])})\n",
        "\n",
        "  for n in range(gen_length):\n",
        "    #Dynamic Input\n",
        "        dyn_input = [ [vocab_to_int[word] for word in gen_sentences[-seq_length:]] ]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "        probabilities, prev_state = sess.run(\n",
        "                [probs, final_state],\n",
        "                {input_text: dyn_input, initial_state: prev_state})\n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "        gen_sentences.append(pred_word)\n",
        "  \n",
        "  tv_script = ' '.join(gen_sentences)\n",
        "  for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "  tv_script = tv_script.replace('\\n ', '\\n')\n",
        "  tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "  print(tv_script)\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-b1bb42e3401c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Dynamic Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdyn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdyn_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyn_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         probabilities, prev_state = sess.run(\n",
            "\u001b[0;32m<ipython-input-74-b1bb42e3401c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Dynamic Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdyn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdyn_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyn_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         probabilities, prev_state = sess.run(\n",
            "\u001b[0;31mKeyError\u001b[0m: 'comet:'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZY3AP_1f-Yj",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XARtdvLNfd_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "\n",
        "def _print_success_message():\n",
        "    print('Tests Passed')\n",
        "\n",
        "\n",
        "def test_create_lookup_tables(create_lookup_tables):\n",
        "    with tf.Graph().as_default():\n",
        "        test_text = '''\n",
        "        Moe_Szyslak Moe's Tavern Where the elite meet to drink\n",
        "        Bart_Simpson Eh yeah hello is Mike there Last name Rotch\n",
        "        Moe_Szyslak Hold on I'll check Mike Rotch Mike Rotch Hey has anybody seen Mike Rotch lately\n",
        "        Moe_Szyslak Listen you little puke One of these days I'm gonna catch you and I'm gonna carve my name on your back with an ice pick\n",
        "        Moe_Szyslak Whats the matter Homer You're not your normal effervescent self\n",
        "        Homer_Simpson I got my problems Moe Give me another one\n",
        "        Moe_Szyslak Homer hey you should not drink to forget your problems\n",
        "        Barney_Gumble Yeah you should only drink to enhance your social skills'''\n",
        "\n",
        "        test_text = test_text.lower()\n",
        "        test_text = test_text.split()\n",
        "\n",
        "        vocab_to_int, int_to_vocab = create_lookup_tables(test_text)\n",
        "\n",
        "        # Check types\n",
        "        assert isinstance(vocab_to_int, dict),\\\n",
        "            'vocab_to_int is not a dictionary.'\n",
        "        assert isinstance(int_to_vocab, dict),\\\n",
        "            'int_to_vocab is not a dictionary.'\n",
        "\n",
        "        # Compare lengths of dicts\n",
        "        assert len(vocab_to_int) == len(int_to_vocab),\\\n",
        "            'Length of vocab_to_int and int_to_vocab don\\'t match. ' \\\n",
        "            'vocab_to_int is length {}. int_to_vocab is length {}'.format(len(vocab_to_int), len(int_to_vocab))\n",
        "\n",
        "        # Make sure the dicts have the same words\n",
        "        vocab_to_int_word_set = set(vocab_to_int.keys())\n",
        "        int_to_vocab_word_set = set(int_to_vocab.values())\n",
        "\n",
        "        assert not (vocab_to_int_word_set - int_to_vocab_word_set),\\\n",
        "            'vocab_to_int and int_to_vocab don\\'t have the same words.' \\\n",
        "            '{} found in vocab_to_int, but not in int_to_vocab'.format(vocab_to_int_word_set - int_to_vocab_word_set)\n",
        "        assert not (int_to_vocab_word_set - vocab_to_int_word_set),\\\n",
        "            'vocab_to_int and int_to_vocab don\\'t have the same words.' \\\n",
        "            '{} found in int_to_vocab, but not in vocab_to_int'.format(int_to_vocab_word_set - vocab_to_int_word_set)\n",
        "\n",
        "        # Make sure the dicts have the same word ids\n",
        "        vocab_to_int_word_id_set = set(vocab_to_int.values())\n",
        "        int_to_vocab_word_id_set = set(int_to_vocab.keys())\n",
        "\n",
        "        assert not (vocab_to_int_word_id_set - int_to_vocab_word_id_set),\\\n",
        "            'vocab_to_int and int_to_vocab don\\'t contain the same word ids.' \\\n",
        "            '{} found in vocab_to_int, but not in int_to_vocab'.format(vocab_to_int_word_id_set - int_to_vocab_word_id_set)\n",
        "        assert not (int_to_vocab_word_id_set - vocab_to_int_word_id_set),\\\n",
        "            'vocab_to_int and int_to_vocab don\\'t contain the same word ids.' \\\n",
        "            '{} found in int_to_vocab, but not in vocab_to_int'.format(int_to_vocab_word_id_set - vocab_to_int_word_id_set)\n",
        "\n",
        "        # Make sure the dicts make the same lookup\n",
        "        missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
        "\n",
        "        assert not missmatches,\\\n",
        "            'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(\n",
        "                len(missmatches),\n",
        "                *missmatches[0])\n",
        "\n",
        "        assert len(vocab_to_int) > len(set(test_text))/2,\\\n",
        "            'The length of vocab seems too small.  Found a length of {}'.format(len(vocab_to_int))\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_get_batches(get_batches):\n",
        "    with tf.Graph().as_default():\n",
        "        test_batch_size = 128\n",
        "        test_seq_length = 5\n",
        "        test_int_text = list(range(1000*test_seq_length))\n",
        "        batches = get_batches(test_int_text, test_batch_size, test_seq_length)\n",
        "\n",
        "        # Check type\n",
        "        assert isinstance(batches, np.ndarray),\\\n",
        "            'Batches is not a Numpy array'\n",
        "\n",
        "        # Check shape\n",
        "        assert batches.shape == (7, 2, 128, 5),\\\n",
        "            'Batches returned wrong shape.  Found {}'.format(batches.shape)\n",
        "\n",
        "        for x in range(batches.shape[2]):\n",
        "            assert np.array_equal(batches[0,0,x], np.array(range(x * 35, x * 35 + batches.shape[3]))),\\\n",
        "                'Batches returned wrong contents. For example, input sequence {} in the first batch was {}'.format(x, batches[0,0,x])\n",
        "            assert np.array_equal(batches[0,1,x], np.array(range(x * 35 + 1, x * 35 + 1 + batches.shape[3]))),\\\n",
        "                'Batches returned wrong contents. For example, target sequence {} in the first batch was {}'.format(x, batches[0,1,x])\n",
        "\n",
        "\n",
        "        last_seq_target = (test_batch_size-1) * 35 + 31\n",
        "        last_seq = np.array(range(last_seq_target, last_seq_target+ batches.shape[3]))\n",
        "        last_seq[-1] = batches[0,0,0,0]\n",
        "\n",
        "        assert np.array_equal(batches[-1,1,-1], last_seq),\\\n",
        "            'The last target of the last batch should be the first input of the first batch. Found {} but expected {}'.format(batches[-1,1,-1], last_seq)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_tokenize(token_lookup):\n",
        "    with tf.Graph().as_default():\n",
        "        symbols = set(['.', ',', '\"', ';', '!', '?', '(', ')', '--', '\\n'])\n",
        "        token_dict = token_lookup()\n",
        "\n",
        "        # Check type\n",
        "        assert isinstance(token_dict, dict), \\\n",
        "            'Returned type is {}.'.format(type(token_dict))\n",
        "\n",
        "        # Check symbols\n",
        "        missing_symbols = symbols - set(token_dict.keys())\n",
        "        unknown_symbols = set(token_dict.keys()) - symbols\n",
        "\n",
        "        assert not missing_symbols, \\\n",
        "            'Missing symbols: {}'.format(missing_symbols)\n",
        "        assert not unknown_symbols, \\\n",
        "            'Unknown symbols: {}'.format(unknown_symbols)\n",
        "\n",
        "        # Check values type\n",
        "        bad_value_type = [type(val) for val in token_dict.values() if not isinstance(val, str)]\n",
        "\n",
        "        assert not bad_value_type,\\\n",
        "            'Found token as {} type.'.format(bad_value_type[0])\n",
        "\n",
        "        # Check for spaces\n",
        "        key_has_spaces = [k for k in token_dict.keys() if ' ' in k]\n",
        "        val_has_spaces = [val for val in token_dict.values() if ' ' in val]\n",
        "\n",
        "        assert not key_has_spaces,\\\n",
        "            'The key \"{}\" includes spaces. Remove spaces from keys and values'.format(key_has_spaces[0])\n",
        "        assert not val_has_spaces,\\\n",
        "            'The value \"{}\" includes spaces. Remove spaces from keys and values'.format(val_has_spaces[0])\n",
        "\n",
        "        # Check for symbols in values\n",
        "        symbol_val = ()\n",
        "        for symbol in symbols:\n",
        "            for val in token_dict.values():\n",
        "                if symbol in val:\n",
        "                    symbol_val = (symbol, val)\n",
        "\n",
        "        assert not symbol_val,\\\n",
        "            'Don\\'t use a symbol that will be replaced in your tokens. Found the symbol {} in value {}'.format(*symbol_val)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_get_inputs(get_inputs):\n",
        "    with tf.Graph().as_default():\n",
        "        input_data, targets, lr = get_inputs()\n",
        "\n",
        "        # Check type\n",
        "        assert input_data.op.type == 'Placeholder',\\\n",
        "            'Input not a Placeholder.'\n",
        "        assert targets.op.type == 'Placeholder',\\\n",
        "            'Targets not a Placeholder.'\n",
        "        assert lr.op.type == 'Placeholder',\\\n",
        "            'Learning Rate not a Placeholder.'\n",
        "\n",
        "        # Check name\n",
        "        assert input_data.name == 'input:0',\\\n",
        "            'Input has bad name.  Found name {}'.format(input_data.name)\n",
        "\n",
        "        # Check rank\n",
        "        input_rank = 0 if input_data.get_shape() == None else len(input_data.get_shape())\n",
        "        targets_rank = 0 if targets.get_shape() == None else len(targets.get_shape())\n",
        "        lr_rank = 0 if lr.get_shape() == None else len(lr.get_shape())\n",
        "\n",
        "        assert input_rank == 2,\\\n",
        "            'Input has wrong rank.  Rank {} found.'.format(input_rank)\n",
        "        assert targets_rank == 2,\\\n",
        "            'Targets has wrong rank. Rank {} found.'.format(targets_rank)\n",
        "        assert lr_rank == 0,\\\n",
        "            'Learning Rate has wrong rank. Rank {} found'.format(lr_rank)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_get_init_cell(get_init_cell):\n",
        "    with tf.Graph().as_default():\n",
        "        test_batch_size_ph = tf.placeholder(tf.int32, [])\n",
        "        test_rnn_size = 256\n",
        "\n",
        "        cell, init_state = get_init_cell(test_batch_size_ph, test_rnn_size)\n",
        "\n",
        "        # Check type\n",
        "        assert isinstance(cell, tf.contrib.rnn.MultiRNNCell),\\\n",
        "            'Cell is wrong type.  Found {} type'.format(type(cell))\n",
        "\n",
        "        # Check for name attribute\n",
        "        assert hasattr(init_state, 'name'),\\\n",
        "            'Initial state doesn\\'t have the \"name\" attribute.  Try using `tf.identity` to set the name.'\n",
        "\n",
        "        # Check name\n",
        "        assert init_state.name == 'initial_state:0',\\\n",
        "            'Initial state doesn\\'t have the correct name. Found the name {}'.format(init_state.name)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_get_embed(get_embed):\n",
        "    with tf.Graph().as_default():\n",
        "        embed_shape = [50, 5, 256]\n",
        "        test_input_data = tf.placeholder(tf.int32, embed_shape[:2])\n",
        "        test_vocab_size = 27\n",
        "        test_embed_dim = embed_shape[2]\n",
        "\n",
        "        embed = get_embed(test_input_data, test_vocab_size, test_embed_dim)\n",
        "\n",
        "        # Check shape\n",
        "        assert embed.shape == embed_shape,\\\n",
        "            'Wrong shape.  Found shape {}'.format(embed.shape)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_build_rnn(build_rnn):\n",
        "    with tf.Graph().as_default():\n",
        "        test_rnn_size = 256\n",
        "        test_rnn_layer_size = 2\n",
        "        test_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(test_rnn_size) for _ in range(test_rnn_layer_size)])\n",
        "\n",
        "        test_inputs = tf.placeholder(tf.float32, [None, None, test_rnn_size])\n",
        "        outputs, final_state = build_rnn(test_cell, test_inputs)\n",
        "\n",
        "        # Check name\n",
        "        assert hasattr(final_state, 'name'),\\\n",
        "            'Final state doesn\\'t have the \"name\" attribute.  Try using `tf.identity` to set the name.'\n",
        "        assert final_state.name == 'final_state:0',\\\n",
        "            'Final state doesn\\'t have the correct name. Found the name {}'.format(final_state.name)\n",
        "\n",
        "        # Check shape\n",
        "        assert outputs.get_shape().as_list() == [None, None, test_rnn_size],\\\n",
        "            'Outputs has wrong shape.  Found shape {}'.format(outputs.get_shape())\n",
        "        assert final_state.get_shape().as_list() == [test_rnn_layer_size, 2, None, test_rnn_size],\\\n",
        "            'Final state wrong shape.  Found shape {}'.format(final_state.get_shape())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_build_nn(build_nn):\n",
        "    with tf.Graph().as_default():\n",
        "        test_input_data_shape = [128, 5]\n",
        "        test_input_data = tf.placeholder(tf.int32, test_input_data_shape)\n",
        "        test_rnn_size = 256\n",
        "        test_embed_dim = 300\n",
        "        test_rnn_layer_size = 2\n",
        "        test_vocab_size = 27\n",
        "        test_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(test_rnn_size) for _ in range(test_rnn_layer_size)])\n",
        "\n",
        "        logits, final_state = build_nn(test_cell, test_rnn_size, test_input_data, test_vocab_size, test_embed_dim)\n",
        "\n",
        "        # Check name\n",
        "        assert hasattr(final_state, 'name'), \\\n",
        "            'Final state doesn\\'t have the \"name\" attribute.  Are you using build_rnn?'\n",
        "        assert final_state.name == 'final_state:0', \\\n",
        "            'Final state doesn\\'t have the correct name. Found the name {}. Are you using build_rnn?'.format(final_state.name)\n",
        "\n",
        "        # Check Shape\n",
        "        assert logits.get_shape().as_list() == test_input_data_shape + [test_vocab_size], \\\n",
        "            'Outputs has wrong shape.  Found shape {}'.format(logits.get_shape())\n",
        "        assert final_state.get_shape().as_list() == [test_rnn_layer_size, 2, 128, test_rnn_size], \\\n",
        "            'Final state wrong shape.  Found shape {}'.format(final_state.get_shape())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_get_tensors(get_tensors):\n",
        "    test_graph = tf.Graph()\n",
        "    with test_graph.as_default():\n",
        "        test_input = tf.placeholder(tf.int32, name='input')\n",
        "        test_initial_state = tf.placeholder(tf.int32, name='initial_state')\n",
        "        test_final_state = tf.placeholder(tf.int32, name='final_state')\n",
        "        test_probs = tf.placeholder(tf.float32, name='probs')\n",
        "\n",
        "    input_text, initial_state, final_state, probs = get_tensors(test_graph)\n",
        "\n",
        "    # Check correct tensor\n",
        "    assert input_text == test_input,\\\n",
        "        'Test input is wrong tensor'\n",
        "    assert initial_state == test_initial_state, \\\n",
        "        'Initial state is wrong tensor'\n",
        "    assert final_state == test_final_state, \\\n",
        "        'Final state is wrong tensor'\n",
        "    assert probs == test_probs, \\\n",
        "        'Probabilities is wrong tensor'\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_pick_word(pick_word):\n",
        "    with tf.Graph().as_default():\n",
        "        test_probabilities = np.array([0.1, 0.8, 0.05, 0.05])\n",
        "        test_int_to_vocab = {word_i: word for word_i, word in enumerate(['this', 'is', 'a', 'test'])}\n",
        "\n",
        "        pred_word = pick_word(test_probabilities, test_int_to_vocab)\n",
        "\n",
        "        # Check type\n",
        "        assert isinstance(pred_word, str),\\\n",
        "            'Predicted word is wrong type. Found {} type.'.format(type(pred_word))\n",
        "\n",
        "        # Check word is from vocab\n",
        "        assert pred_word in test_int_to_vocab.values(),\\\n",
        "            'Predicted word not found in int_to_vocab.'\n",
        "\n",
        "\n",
        "    _print_success_message()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vWIMLPp4KbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}